{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scandi_languages.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xILUvrM-Ag-Z"
      },
      "source": [
        "import string\r\n",
        "import random\r\n",
        "import time\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import torch as pt\r\n",
        "import torch.nn as nn\r\n",
        "from xml.dom import minidom"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vosGmR83R5-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa44585-611a-41d2-fe5d-c66a9222376c"
      },
      "source": [
        "num_files = 30\r\n",
        "\r\n",
        "da_p = []\r\n",
        "for k in range(num_files):\r\n",
        "  filename = 'da (%d).xml' % (k+1)\r\n",
        "  da_1 = minidom.parse(filename)\r\n",
        "  da1_text = da_1.getElementsByTagName('s')\r\n",
        "  dq = len(da1_text)\r\n",
        "  for j in range(dq):\r\n",
        "    dp = len(da1_text[j].childNodes)\r\n",
        "    for i in range(dp):\r\n",
        "      if da1_text[j].childNodes[i].nodeType == 1:\r\n",
        "        continue\r\n",
        "      elif da1_text[j].childNodes[i].nodeValue == '\\n    ':\r\n",
        "        continue\r\n",
        "      elif da1_text[j].childNodes[i].nodeValue == '\\n  ':\r\n",
        "        continue\r\n",
        "      else:\r\n",
        "        da_p.append(da1_text[j].childNodes[i].nodeValue)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "sv_p = []\r\n",
        "for k in range(num_files):\r\n",
        "  filename = 'sv (%d).xml' % (k+1)\r\n",
        "  sv_1 = minidom.parse(filename)\r\n",
        "  sv1_text = sv_1.getElementsByTagName('s')\r\n",
        "  sq = len(sv1_text)\r\n",
        "  for j in range(sq):\r\n",
        "    sp = len(sv1_text[j].childNodes)\r\n",
        "    for i in range(sp):\r\n",
        "      if sv1_text[j].childNodes[i].nodeType == 1:\r\n",
        "        continue\r\n",
        "      elif sv1_text[j].childNodes[i].nodeValue == '\\n    ':\r\n",
        "        continue\r\n",
        "      elif sv1_text[j].childNodes[i].nodeValue == '\\n  ':\r\n",
        "        continue\r\n",
        "      else:\r\n",
        "        sv_p.append(sv1_text[j].childNodes[i].nodeValue)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "no_p = []\r\n",
        "for k in range(num_files):\r\n",
        "  filename = 'no (%d).xml' % (k+1)\r\n",
        "  no_1 = minidom.parse(filename)\r\n",
        "  no1_text = no_1.getElementsByTagName('s')\r\n",
        "  nq = len(no1_text)\r\n",
        "  for j in range(nq):\r\n",
        "    nop = len(no1_text[j].childNodes)\r\n",
        "    for i in range(nop):\r\n",
        "      if no1_text[j].childNodes[i].nodeType == 1:\r\n",
        "        continue\r\n",
        "      elif no1_text[j].childNodes[i].nodeValue == '\\n    ':\r\n",
        "        continue\r\n",
        "      elif no1_text[j].childNodes[i].nodeValue == '\\n  ':\r\n",
        "        continue\r\n",
        "      else:\r\n",
        "        no_p.append(no1_text[j].childNodes[i].nodeValue)\r\n",
        "\r\n",
        "\r\n",
        "da_len = len(da_p)\r\n",
        "sv_len = len(sv_p)\r\n",
        "no_len = len(no_p)\r\n",
        "\r\n",
        "da_fl = []\r\n",
        "sv_fl = []\r\n",
        "no_fl =[]\r\n",
        "\r\n",
        "for i in range(da_len):\r\n",
        "  da_fl.append(da_p[i].split())\r\n",
        "\r\n",
        "for i in range(sv_len):\r\n",
        "  sv_fl.append(sv_p[i].split())\r\n",
        "\r\n",
        "for i in range(no_len):\r\n",
        "  no_fl.append(no_p[i].split())\r\n",
        "\r\n",
        "da = [item for sublist in da_fl for item in sublist]\r\n",
        "sv = [item for sublist in sv_fl for item in sublist]\r\n",
        "no = [item for sublist in no_fl for item in sublist]\r\n",
        "\r\n",
        "da_l = len(da)\r\n",
        "sv_l = len(sv)\r\n",
        "no_l = len(no)\r\n",
        "\r\n",
        "for i in range(da_l):\r\n",
        "  if da[i].isalpha() == False:\r\n",
        "    da[i] = \"\"\r\n",
        "  else:\r\n",
        "    da[i] = da[i].lower()\r\n",
        "\r\n",
        "for i in range(sv_l):\r\n",
        "  if sv[i].isalpha() == False:\r\n",
        "    sv[i] = \"\"\r\n",
        "  else:\r\n",
        "    sv[i] = sv[i].lower()\r\n",
        "\r\n",
        "for i in range(no_l):\r\n",
        "  if no[i].isalpha() == False:\r\n",
        "    no[i] = \"\"\r\n",
        "  else:\r\n",
        "    no[i] = no[i].lower()\r\n",
        "\r\n",
        "\r\n",
        "filter_object_d = filter(lambda x: x != \"\", da)\r\n",
        "filter_object_s = filter(lambda x: x != \"\", sv)\r\n",
        "filter_object_n = filter(lambda x: x != \"\", no)\r\n",
        "da = list(filter_object_d)\r\n",
        "sv = list(filter_object_s)\r\n",
        "no = list(filter_object_n)\r\n",
        "\r\n",
        "#da = list(dict.fromkeys(da))\r\n",
        "#sv = list(dict.fromkeys(sv))\r\n",
        "#no = list(dict.fromkeys(no))\r\n",
        "\r\n",
        "#i = set(da).intersection(set(sv))\r\n",
        "#j = set(da).intersection(set(no))\r\n",
        "#k = set(sv).intersection(set(no))\r\n",
        "#ijk = i.intersection(set(no))\r\n",
        "\r\n",
        "#aa = list(set(da).difference(ijk))\r\n",
        "#bb = list(set(sv).difference(ijk))\r\n",
        "#cc = list(set(no).difference(ijk))\r\n",
        "\r\n",
        "#aaa = list(set(aa).difference(i))\r\n",
        "#da = list(set(aaa).difference(j))\r\n",
        "\r\n",
        "#bbb = list(set(bb).difference(i))\r\n",
        "#sv = list(set(bbb).difference(k))\r\n",
        "\r\n",
        "#ccc = list(set(cc).difference(j))\r\n",
        "#no = list(set(ccc).difference(k))\r\n",
        "\r\n",
        "print(da[:100])\r\n",
        "print(sv[:100])\r\n",
        "print(no[:100])\r\n",
        "print(len(da))\r\n",
        "print(len(sv))\r\n",
        "print(len(no))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mere', 'stav', 'det', 'staves', 'sådan', 'godt', 'i', 'lige', 'land', 'of', 'the', 'home', 'of', 'the', 'flot', 'jeg', 'skal', 'nok', 'lære', 'dig', 'at', 'jeg', 'sidder', 'jeg', 'er', 'så', 'jeg', 'sidder', 'jeg', 'er', 'og', 'skal', 'lære', 'dig', 'det', 'er', 'som', 'at', 'flyve', 'jeg', 'er', 'vant', 'til', 'du', 'tager', 'jeg', 'giver', 'nu', 'tager', 'vi', 'de', 'dumme', 'hæng', 'jeg', 'har', 'set', 'far', 'gøre', 'det', 'masser', 'af', 'vær', 'ikke', 'kors', 'i', 'vi', 'vi', 'far', 'slår', 'mig', 'vi', 'vi', 'jeg', 'er', 'jeg', 'ved', 'vi', 'din', 'hvis', 'du', 'leger', 'den', 'der', 'så', 'bliver', 'du', 'aldrig', 'til', 'han', 'er', 'ikke', 'kan', 'du', 'så', 'komme', 'rejs', 'dig', 'arbejdet', 'gå', 'hjem']\n",
            "['flicka', 'nr', 'shiguse', 'pojke', 'nr', 'kazushi', 'flicka', 'nr', 'takako', 'totalt', 'tre', 'ni', 'börjar', 'slöa', 'jag', 'är', 'besviken', 'på', 'därför', 'lägger', 'jag', 'till', 'fler', 'en', 'ny', 'varje', 'så', 'hör', 'noga', 'från', 'från', 'från', 'det', 'är', 'pappa', 'är', 'tack', 'för', 'vill', 'du', 'nej', 'jag', 'tänkte', 'inte', 'jag', 'vill', 'att', 'ni', 'ska', 'hon', 'är', 'är', 'ni', 'lika', 'vi', 'gick', 'i', 'samma', 'nionde', 'klass', 'på', 'kobe', 'junior', 'vi', 'var', 'här', 'för', 'tre', 'år', 'då', 'är', 'jag', 'överlevde', 'det', 'här', 'förbannade', 'jag', 'gjorde', 'allt', 'för', 'att', 'skydda', 'jag', 'dödade', 'en', 'kompis', 'så', 'att', 'vi', 'två', 'skulle', 'kunna', 'när', 'bara', 'en', 'av', 'oss', 'kunde', 'överleva', 'hände']\n",
            "['desember', 'hamrer', 'de', 'allierte', 'styrkene', 'løs', 'mot', 'tyskerne', 'med', 'ubøyelig', 'vi', 'regner', 'ikke', 'med', 'å', 'få', 'en', 'vinterferie', 'i', 'vi', 'regner', 'med', 'å', 'fortsette', 'og', 'bataljonshovedkvarter', 'korps', 'v', 'desember', 'og', 'ukjent', 'med', 'de', 'var', 'bare', 'nåler', 'på', 'et', 'kart', 'for', 'prøver', 'du', 'å', 'score', 'noen', 'prøver', 'bare', 'å', 'styrke', 'kapteinen', 'trenger', 'skyss', 'til', 'kan', 'du', 'finne', 'en', 'sjåfør', 'til', 'jeg', 'tenkte', 'at', 'du', 'ville', 'si', 'jeg', 'ser', 'generalen', 'bør', 'bli', 'glad', 'for', 'nå', 'langs', 'en', 'kamplinje', 'på', 'kilometer', 'i', 'og', 'har', 'de', 'allierte', 'sluppet', 'ned', 'to', 'millioner', 'og', 'en', 'vei', 'eller', 'to', 'som', 'ikke', 'var', 'brolagt', 'med', 'er', 'det', 'obersten']\n",
            "147724\n",
            "126836\n",
            "137161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYcYVhCR0dqu",
        "outputId": "5f299432-8c9e-419c-b319-d06cda1b3aa0"
      },
      "source": [
        "category_words = {'Danish':da,'Swedish':sv,'Norwegian':no}\r\n",
        "all_categories = ['Danish','Swedish','Norwegian']\r\n",
        "\r\n",
        "print(category_words['Norwegian'][:5])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['desember', 'hamrer', 'de', 'allierte', 'styrkene']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Ak8UWyjkL-",
        "outputId": "e3390bbd-65ca-4703-e83c-fe09104ddfc9"
      },
      "source": [
        "all_letters = string.ascii_lowercase + \"æøåäö\"\r\n",
        "n_letters = len(all_letters)\r\n",
        "print(n_letters)\r\n",
        "\r\n",
        "def letterToIndex(letter):\r\n",
        "    return all_letters.find(letter)\r\n",
        "\r\n",
        "print(letterToIndex('å'))\r\n",
        "\r\n",
        "def wordToTensor(word):\r\n",
        "    tensor = pt.zeros(len(word), 1, n_letters)\r\n",
        "    for li, letter in enumerate(word):\r\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\r\n",
        "    return tensor\r\n",
        "\r\n",
        "print(wordToTensor('hello'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "28\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyNDbzZcyT8J",
        "outputId": "2df3a12d-fed6-4551-97ba-085a8f1b5b7e"
      },
      "source": [
        "class RNN(nn.Module):\r\n",
        "  def __init__(self,input_size,hidden_size,output_size):\r\n",
        "    super(RNN,self).__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\r\n",
        "    self.i2o = nn.Linear(input_size + hidden_size, output_size)\r\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "  def forward(self, input, hidden):\r\n",
        "    combined = pt.cat((input, hidden), 1)\r\n",
        "    hidden = self.i2h(combined)\r\n",
        "    output = self.i2o(combined)\r\n",
        "    output = self.softmax(output)\r\n",
        "    return output, hidden\r\n",
        "\r\n",
        "  def initHidden(self):\r\n",
        "    return pt.zeros(1, self.hidden_size)\r\n",
        "\r\n",
        "\r\n",
        "n_hidden = 24\r\n",
        "n_categories = 3\r\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)\r\n",
        "\r\n",
        "input = wordToTensor('hej')\r\n",
        "hidden = pt.zeros(1, n_hidden)\r\n",
        "\r\n",
        "output, next_hidden = rnn(input[0], hidden)\r\n",
        "print(output)\r\n",
        "\r\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.1284, -1.2309, -0.9561]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y5EZ5GG3HND",
        "outputId": "58e735cd-dbac-4805-c8a1-d1a23d6bca82"
      },
      "source": [
        "def categoryFromOutput(output):\r\n",
        "    _, top_i = output.topk(1)\r\n",
        "    category_i = top_i[0].item()\r\n",
        "    return all_categories[category_i], category_i\r\n",
        "\r\n",
        "print(categoryFromOutput(output))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Norwegian', 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k07MmEZp3kxy",
        "outputId": "6a5deecf-2f52-4f25-a390-f82885ebc295"
      },
      "source": [
        "def randomChoice(w):\r\n",
        "    return w[random.randint(0, len(w) - 1)]\r\n",
        "\r\n",
        "def randomTrainingExample():\r\n",
        "    category = randomChoice(all_categories)\r\n",
        "    word = randomChoice(category_words[category])\r\n",
        "    category_tensor = pt.tensor([all_categories.index(category)], dtype=pt.long)\r\n",
        "    word_tensor = wordToTensor(word)\r\n",
        "    return category, word, category_tensor, word_tensor\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "    category, word, category_tensor, word_tensor = randomTrainingExample()\r\n",
        "    print('category =', category, '/ word =', word)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category = Swedish / word = tänker\n",
            "category = Danish / word = som\n",
            "category = Norwegian / word = små\n",
            "category = Danish / word = også\n",
            "category = Norwegian / word = det\n",
            "category = Norwegian / word = kan\n",
            "category = Norwegian / word = hvor\n",
            "category = Danish / word = du\n",
            "category = Swedish / word = inte\n",
            "category = Danish / word = i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kknYGs5O68Qw"
      },
      "source": [
        "criterion = nn.NLLLoss()\r\n",
        "\r\n",
        "learning_rate = 0.01 # If you set this too high, it might explode. If too low, it might not learn\r\n",
        "\r\n",
        "def train(category_tensor, word_tensor):\r\n",
        "    hidden = rnn.initHidden()\r\n",
        "\r\n",
        "    rnn.zero_grad()\r\n",
        "\r\n",
        "    for i in range(word_tensor.size()[0]):\r\n",
        "        output, hidden = rnn(word_tensor[i], hidden)\r\n",
        "\r\n",
        "    loss = criterion(output, category_tensor)\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # Add parameters' gradients to their values, multiplied by learning rate\r\n",
        "    for p in rnn.parameters():\r\n",
        "        p.data.add_(p.grad.data, alpha=-learning_rate)\r\n",
        "\r\n",
        "    return output, loss.item()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07uSG5Jv7RvX",
        "outputId": "ce84e5b2-c5d9-4488-fadb-0ab97c569103"
      },
      "source": [
        "n_iters = 300000\r\n",
        "print_every = 15000\r\n",
        "\r\n",
        "def timeSince(since):\r\n",
        "    now = time.time()\r\n",
        "    s = now - since\r\n",
        "    m = math.floor(s / 60)\r\n",
        "    s -= m * 60\r\n",
        "    return '%dm %ds' % (m, s)\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "for iter in range(1, n_iters + 1):\r\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\r\n",
        "    output, loss = train(category_tensor, line_tensor)\r\n",
        "\r\n",
        "    # Print iter number, loss, name and guess\r\n",
        "    if iter % print_every == 0:\r\n",
        "        guess, guess_i = categoryFromOutput(output)\r\n",
        "        correct = '✓' if guess == category else '✗ (%s)' % category\r\n",
        "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\r\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15000 5% (0m 16s) 0.4178 klockan / Swedish ✓\n",
            "30000 10% (0m 33s) 0.6776 joe / Danish ✓\n",
            "45000 15% (0m 50s) 1.2244 med / Danish ✗ (Swedish)\n",
            "60000 20% (1m 6s) 0.4943 barnepige / Danish ✓\n",
            "75000 25% (1m 23s) 1.1877 synes / Danish ✗ (Norwegian)\n",
            "90000 30% (1m 40s) 1.1277 du / Swedish ✗ (Norwegian)\n",
            "105000 35% (1m 56s) 0.7363 var / Swedish ✓\n",
            "120000 40% (2m 13s) 0.8224 jeg / Danish ✓\n",
            "135000 45% (2m 30s) 1.1196 og / Danish ✗ (Norwegian)\n",
            "150000 50% (2m 47s) 1.9036 ville / Swedish ✗ (Norwegian)\n",
            "165000 55% (3m 3s) 0.9615 udnyttet / Danish ✓\n",
            "180000 60% (3m 20s) 1.2054 i / Norwegian ✗ (Swedish)\n",
            "195000 65% (3m 37s) 0.8105 rejse / Norwegian ✗ (Danish)\n",
            "210000 70% (3m 53s) 1.6309 som / Swedish ✗ (Norwegian)\n",
            "225000 75% (4m 10s) 0.6622 denne / Norwegian ✓\n",
            "240000 80% (4m 27s) 1.1367 deg / Danish ✗ (Norwegian)\n",
            "255000 85% (4m 43s) 1.1406 de / Danish ✗ (Norwegian)\n",
            "270000 90% (5m 0s) 0.4764 angreb / Danish ✓\n",
            "285000 95% (5m 17s) 2.1918 fra / Norwegian ✗ (Danish)\n",
            "300000 100% (5m 33s) 1.5262 pork / Norwegian ✗ (Danish)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xII0AD1F8ECn"
      },
      "source": [
        "def evaluate(word_tensor):\r\n",
        "    hidden = rnn.initHidden()\r\n",
        "\r\n",
        "    for i in range(word_tensor.size()[0]):\r\n",
        "        output, hidden = rnn(word_tensor[i], hidden)\r\n",
        "\r\n",
        "    return output"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7gltJu18EIY",
        "outputId": "78cbdc65-91e5-4787-af11-1631786bfe31"
      },
      "source": [
        "def predict_word(input_word, n_predictions=3):\r\n",
        "    with pt.no_grad():\r\n",
        "        output = evaluate(wordToTensor(input_word))\r\n",
        "\r\n",
        "        # Get top N categories\r\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\r\n",
        "        predictions = []\r\n",
        "\r\n",
        "        for i in range(n_predictions):\r\n",
        "            value = topv[0][i].item()\r\n",
        "            category_index = topi[0][i].item()\r\n",
        "            predictions.append([value, all_categories[category_index]])\r\n",
        "        return predictions\r\n",
        "\r\n",
        "print(predict_word('forhåbentlig'))\r\n",
        "print(predict_word('är'))\r\n",
        "print(predict_word('jeg'))\r\n",
        "print(predict_word('nytår'))\r\n",
        "print(predict_word('tak'))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.8828323483467102, 'Swedish'], [-0.9722431302070618, 'Danish'], [-1.5694644451141357, 'Norwegian']]\n",
            "[[-0.005772349424660206, 'Swedish'], [-5.498067378997803, 'Norwegian'], [-6.400273323059082, 'Danish']]\n",
            "[[-0.7100821137428284, 'Norwegian'], [-1.0515811443328857, 'Danish'], [-1.8387799263000488, 'Swedish']]\n",
            "[[-0.8457363247871399, 'Swedish'], [-1.1397275924682617, 'Norwegian'], [-1.3828892707824707, 'Danish']]\n",
            "[[-0.7828159928321838, 'Norwegian'], [-1.1235649585723877, 'Danish'], [-1.5243415832519531, 'Swedish']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaojGxoG9LJ1",
        "outputId": "4d9365bb-1469-4da7-d88e-4ae0aa4d95fd"
      },
      "source": [
        "def predict(sentence):\r\n",
        "  print('\\n> %s' % sentence)\r\n",
        "  lst = sentence.split()\r\n",
        "  p = len(lst)\r\n",
        "  d = []\r\n",
        "  n = []\r\n",
        "  s = []\r\n",
        "  for i in range(p):\r\n",
        "    x = predict_word(lst[i])\r\n",
        "    if x[0][1] == 'Danish':\r\n",
        "      d.append(x[0][0])\r\n",
        "      if x[1][1] == 'Norwegian':\r\n",
        "        n.append(x[1][0])\r\n",
        "        s.append(x[2][0])\r\n",
        "      else:\r\n",
        "        n.append(x[2][0])\r\n",
        "        s.append(x[1][0])\r\n",
        "    elif x[0][1] == 'Norwegian':\r\n",
        "      n.append(x[0][0])\r\n",
        "      if x[1][1] == 'Danish':\r\n",
        "        d.append(x[1][0])\r\n",
        "        s.append(x[2][0])\r\n",
        "      else:\r\n",
        "        d.append(x[2][0])\r\n",
        "        s.append(x[1][0])\r\n",
        "    else:\r\n",
        "      s.append(x[0][0])\r\n",
        "      if x[1][1] == 'Danish':\r\n",
        "        d.append(x[1][0])\r\n",
        "        n.append(x[2][0])\r\n",
        "      else:\r\n",
        "        d.append(x[2][0])\r\n",
        "        n.append(x[1][0])\r\n",
        "\r\n",
        "  d_av = np.mean(d)\r\n",
        "  print('Danish: %s' % d_av)\r\n",
        "  n_av = np.mean(n)\r\n",
        "  print('Norwegian: %s' % n_av)\r\n",
        "  s_av = np.mean(s)\r\n",
        "  print('Swedish: %s' % s_av)\r\n",
        "\r\n",
        "  if d_av > n_av and d_av > s_av:\r\n",
        "    print('Conclusion: Danish')\r\n",
        "  elif n_av > d_av and n_av > s_av:\r\n",
        "    print('Conclusion: Norwegian')\r\n",
        "  else:\r\n",
        "    print('Conclusion: Swedish')\r\n",
        "\r\n",
        "predict('jeg købte fem æbler i butikken')\r\n",
        "predict('jeg gikk til butikken og kjøpte fem epler')\r\n",
        "predict('jag gick till affären och köpte fem äpplen')\r\n",
        "predict('i dag er det sophias fødselsdag og hun elsker mig så meget')\r\n",
        "predict('på loftet sidder nissen med sin julegrød sin julegrød så god og sød han nikker og han smiler og han er så glade for julegrød er hans bedste mad')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> jeg købte fem æbler i butikken\n",
            "Danish: -0.9228038638830185\n",
            "Norwegian: -0.8019592861334482\n",
            "Swedish: -2.930897533893585\n",
            "Conclusion: Norwegian\n",
            "\n",
            "> jeg gikk til butikken og kjøpte fem epler\n",
            "Danish: -1.006681703031063\n",
            "Norwegian: -0.6874152701348066\n",
            "Swedish: -2.5189815163612366\n",
            "Conclusion: Norwegian\n",
            "\n",
            "> jag gick till affären och köpte fem äpplen\n",
            "Danish: -2.6467357128858566\n",
            "Norwegian: -2.5580000057816505\n",
            "Swedish: -0.6420857610646635\n",
            "Conclusion: Swedish\n",
            "\n",
            "> i dag er det sophias fødselsdag og hun elsker mig så meget\n",
            "Danish: -0.9042764554421107\n",
            "Norwegian: -1.0654096404711406\n",
            "Swedish: -1.9504667818546295\n",
            "Conclusion: Danish\n",
            "\n",
            "> på loftet sidder nissen med sin julegrød sin julegrød så god og sød han nikker og han smiler og han er så glade for julegrød er hans bedste mad\n",
            "Danish: -0.8457870514228426\n",
            "Norwegian: -1.0570176963148445\n",
            "Swedish: -2.1550123393535614\n",
            "Conclusion: Danish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fWp-Ifpzaid"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}